\documentclass[a4paper, twocolumn]{article}
\usepackage[pdftex, hidelinks]{hyperref}

\usepackage{bm}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{courier}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{mathtools}
\usepackage{amssymb}
\lstset{basicstyle=\footnotesize\ttfamily,
        breakatwhitespace = false,
        breaklines = true,
        keepspaces = true,
        language = R,
        showspaces = false,
        showstringspaces = false,
        belowcaptionskip = \bigskipamount,
        framerule = 0.80pt,
        frame = tb,
        belowskip = \bigskipamount,
        escapeinside={<@}{@>}}

\title{TDDE01 -- Machine Learning \\
       Group 9 Laboration Report 3}
\author{{Martin Estgren \texttt{<mares480>}} \\
        {Erik S. V. Jansson \texttt{<erija578>}} \\
        {Sebastian Maghsoudi \texttt{<sebma654>}} \\~\\
        {Link√∂ping University (LiU), Sweden}}

\begin{document}
    \pagenumbering{arabic}
    \maketitle % Generate.

    \section*{Assignment 1}

        In this assignment we are tasked with predicting the sex of Australian Crabs given their carapace length and the rear width. For this purpose we will be using
        Linear Discriminant Analyis (LDA) using Maximum Likelihood Estimation for predicting the unknown classification parameters. The first step involves plotting the
        data in order to determine if the data would be linearly separable into different categories (male and female in this case). This plot can be found in below in
        Figure~\ref{fig:crabs}. As can be seen, if we draw a line between these two colored classifications, we could do so such that most \emph{male} and \emph{female}
        classes are separeted. We want to find this line, called the \emph{decision boundary}.

        \begin{figure}[h!]
          \centering
          \caption{Sex Classification of Australian Crabs}
          \label{fig:crabs}
          \includegraphics[width=0.5\textwidth]{share/crabs.eps}
        \end{figure}

        Our next task is to visualize the decision boundary between the different classifications. Since this is not already implemented by a package in R, we have to
        develop our own LDA method. A LDA can be implemented by comparing Linear Discriminant Functions (LDF) with each other, one for each corresponding class. In practice
        this means, since LDF is implemented with a likelihood function, one can assume that by applying predictor features to the function, the result would yield some indication
        of how likely this observation can be classified as the label corresponding to that LDF. An LDF can be described in the following way:

        \begin{equation} \label{eq:lda}
          \begin{split}
            \delta_k(x) &=  w_1 - b_1 \\
            w_1 &= x^{T}\Sigma^{-1}\mu_k, \\
            b_1 &= \frac{1}{2}\mu_k^{T}\Sigma^{-1}\mu_k+log\pi_k
          \end{split}
        \end{equation}

        After calculating the parameters for both labels (male and female) using the Equation~\ref{eq:lda} above, we can find the intercept and slope of the decision boundary
        separating both of these classes. This is done by simply subtracting each coefficient from each other, thereafter plotting this in Figure~\ref{fig:boundary}. Intuitively this
        is a short method of comparing likelihood of classes given feature predictors. All of these calculations (more importantly, Equation~\ref{eq:lda} can be found in Listing~\ref{lst:lda},
        where the function \texttt{lda} calculates the parameters $\mathbf{w}_m$ and $\mathbf{w}_f$, for each respective class, \emph{male} and \emph{female classes} in this case.

        \begin{figure}
          \centering
          \caption{Sex Classification of Australian Crabs}
          \label{fig:boundary}
          \includegraphics[width=0.5\textwidth]{share/boundary.eps}
        \end{figure}

       The final part is to compare the result received by implementing the LDA method with the result from using logistic regression. In r, there is package for this called \textit{glm}. 
	   The function \textit{glm()} returns a vector containing the coefficients including the intersection. The result can be seen in figure \ref{fig:boundarylr}. As the observer can see, there
	   is no significant difference between the result of the method, at least when it come to classify the sample observations, however, there are still a couple of observations which have been
       missclassified.

        \begin{figure}
          \centering
            \caption{Sex Classification of Australian Crabs}
          \label{fig:boundarylr}
          \includegraphics[width=0.5\textwidth]{share/boundarylr.eps}
        \end{figure}

    \section*{Assignment 2}
	This assignment revolved around classification of the credit score of clients. The response depended on some other factors commonly linked with what lenders evaluate before giving credit. 
	The credit score of clients could be divided simply in good or bad.\newline
    The methods that is appropriate for this are prediction through a decision tree or a naive Bayesian model. The first model to evaluate is the decision tree. A decision tree simply create  
	model that depending on the value of some attributes can predict the response. The attributes is ranked in a tree form, where the more definitive attributes tend to be at the top. When the 
	the model is used for prediction the algorithm simply traverse the tree and following which path corresponds to the values for the corresponding attribute.

	The sample observations are randomly split into three parts with the size:
	\begin{equation}
		\left | testing \right | = \frac{\left | data \right |}{2}
	\end{equation}
	\begin{equation}
		 \left | training \right | = \frac{\left | data \right |}{4}
	\end{equation}
	\begin{equation}
	 \left | validation \right | = \frac{\left | data \right |}{4}
	\end{equation}

	\subsection{Decision tree}

	The \textit{training} set is used to train two decision tree models. One with \textit{deviance} and one with \textit{gini index} as metric. The resulting trees can be observed in the figures \ref{fig:deviance tree}/\ref{fig:gini tree} respectively.
	\begin{figure}
	    \centering
		\includegraphics[width=0.5\textwidth]{share/deviance_tree.png}  
		\caption{\textit{Deviance} decision tree.\label{fig:deviance tree}}
	\end{figure}
	\begin{figure}
	    \centering
		\includegraphics[width=0.5\textwidth]{share/gini_tree.png}  
		\caption{\textit{gini index} decision tree.\label{fig:gini tree}}
    \end{figure}
    The \textit{gini index} tree have significantly increased complexity compared to the \textit{deviance} and, when predicting the \textit{testing} data slightly higher misclassification rate.
	\begin{verbatim}
	deviance model fitness
	       predicted
	       bad good
	  bad   29   17
	  good  45  159
	misclassification rate: 0.248
	\end{verbatim}
	\begin{verbatim}
	gini model fitness
	       predicted
	       bad good
	  bad   24   26
	  good  50  150
	misclassification rate: 0.304
	\end{verbatim}
	The \textit{deviance} model is decided to be the better model.

	In order to determine the best max tree depth of the \textit{deviance} model we iterates through all depths between 2 and 15.
	\begin{figure}
	\centering
	\begin{minipage}[]{0.3\textwidth}
	  \includegraphics[width=\textwidth]{share/depth_tree.png}  
	  \caption{deviance for different max tree depths.\label{fig:tree_depth}}
	 \end{minipage}
	\end{figure}
	The \textit{red} line indicates deviance of the training data and the \textit{blue} line, the validation data. Tree depth of 4 results in the least deviance for the validation data. With a max tree depth of 4 the following classification table is produced.
	\begin{verbatim}
	optimal depth deviance fitness
	       predicted
	       bad good
	  bad   20   59
	  good   9  162
	misclassification rate: 0.272
	\end{verbatim}
	The misclassification rate is slightly lower than the \textit{gini index}. The tree is ploted in figure \ref{fig:best_subtree}.
    \begin{figure}[h!]
		\includegraphics[width=0.5\textwidth]{share/best_subtree.png}  
		\caption{\textit{Deviance} decision tree at max depth 4.\label{fig:best_subtree}}
	\end{figure}

	\subsection{Naive Bayes}

        Now we proceed to fit another model with the same problem as before, but using the \emph{Na{\"\i}ve Bayes classifier} instead, which basically assumes that all \emph{features} are independent of each other, and don't produce a conditional chain of dependencies, essentially following the properties: $p(C_k | x_1, x_2, ..., x_n) \propto p(x_i | C_k)$, proportional to $x_i$.

        Under lines \texttt{25-33} in Listing~\ref{lst:bayes} we fit the model with \texttt{e1071}'s \texttt{naiveBayes} \emph{classification}, and produce the \emph{confusion matrices} and \emph{misclassifications} below for both the \emph{training} and the \emph{testing data sets}, which are quite different from decision trees.

        \emph{Misclassification:} \emph{train} = 0.300 \& \emph{test} = 0.306.


        \begin{table}[h]
        \begin{center}
        \begin{tabular}{r|c|c|}
            \multicolumn{1}{r}{\emph{train}}
            &\multicolumn{1}{c}{\textbf{bad}}
            &\multicolumn{1}{c}{\textbf{good}} \\
            \cline{2-3}
            \textbf{bad} & 95 & 98 \\
            \cline{2-3}
            \textbf{good} & 52 & 255 \\
            \cline{2-3}
        \end{tabular}
        \begin{tabular}{r|c|c|}
            \multicolumn{1}{r}{\emph{test}}
            &\multicolumn{1}{c}{\textbf{bad}}
            &\multicolumn{1}{c}{\textbf{good}} \\
            \cline{2-3}
            \textbf{bad} & 142 & 147 \\
            \cline{2-3}
            \textbf{good} & 83 & 378 \\
            \cline{2-3}
        \end{tabular}
        \end{center}
        \caption{Normal Na{\"\i}ve Bayes Confusion Matrix}
        \label{table:bayes}
        \end{table}

        It seems that in this case, \emph{Na{\"\i}ve Bayes} performs worse than \emph{decision trees} since the misclassification rate is quite a bit higher and gives a lot more false negatives and false positives as seen in the confusion matrices above. One possible cause for this is that the model isn't being penalized enough for making the wrong prediction, we need to guide it.

        Now, let's assume that we are given a \emph{loss matrix}, a way of \emph{penalizing} the \emph{predictor} to apply weight to certain false predictions, which in this case is $L = \left( \begin{smallmatrix}0&1\\10&0\end{smallmatrix} \right)$. Doing this in lines \texttt{35-52} within Listing~\ref{lst:bayes} results in a prediction giving the \emph{confusion matrix} below and misclassifications of train = 0.274 and test = 0.278. These misclassifications are a lot lower than those in the normal ``lossless'' model. The reason why this happens is because the cost of doing a faulty classification is highly penalized in one case (while the other isn't penalized as much), this is a underlying reason for these results.


        \begin{table}[h]
        \begin{center}
        \begin{tabular}{r|c|c|}
            \multicolumn{1}{r}{\emph{train}}
            &\multicolumn{1}{c}{\textbf{bad}}
            &\multicolumn{1}{c}{\textbf{good}} \\
            \cline{2-3}
            \textbf{bad} & 27 & 17 \\
            \cline{2-3}
            \textbf{good} & 120 & 336 \\
            \cline{2-3}
        \end{tabular}
        \begin{tabular}{r|c|c|}
            \multicolumn{1}{r}{\emph{test}}
            &\multicolumn{1}{c}{\textbf{bad}}
            &\multicolumn{1}{c}{\textbf{good}} \\
            \cline{2-3}
            \textbf{bad} & 40 & 24 \\
            \cline{2-3}
            \textbf{good} & 185 & 501 \\
            \cline{2-3}
        \end{tabular}
        \end{center}
        \caption{``Lossy'' Na{\"\i}ve Bayes Confusion Matrix}
        \label{table:bayes10}
        \end{table}

        \section*{Contributions}
            \begin{itemize}
                \item{\textbf{Martin Estgren:} contributed several sections in \emph{Assignment 2}, most notably \emph{Decision Trees} and all of the \emph{Figures} there. He also contributed code for the same part of \emph{Assignment 2}.}
                \item{\textbf{Sebastian Maghsoudi:} contributed by writing most of the theory and explanations in \emph{Assignment 1} and presenting Equation~\ref{lst:lda}. Additionally, he also wrote the initial portion of Assignment 2 up and until the division of the samples.}
                \item{\textbf{Erik S. V. Jansson:} assisted in writing parts of \emph{Assignment 1} along with \emph{Sebastian} and also wrote the section on \emph{Naive Bayes classifiers}.}
            \end{itemize}

    \nocite{*} % No warnings.
    \bibliographystyle{alpha}
    \bibliography{report}
    \onecolumn \appendix
    \section*{Appendix}

    \lstinputlisting[caption={Na{\"\i}ve Bayes Part of Assignment 2},label={lst:bayes}]{../share/bayes.r}
    \lstinputlisting[caption={Linear Discriminant Analysis Assignment},label={lst:lda}]{../share/lda.r}
    \lstinputlisting[caption={Decision Tree Part of Assignment 2},label={lst:trees}]{../share/trees.r}

\end{document}
